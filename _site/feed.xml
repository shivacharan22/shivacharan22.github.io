<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-09-09T18:05:31-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">shivacharan oruganti</title><subtitle>Personal website of shivacharan oruganti, a grad student at Boston University.
</subtitle><author><name>shivacharan oruganti</name><email>shivaco@bu.edu</email></author><entry><title type="html">Vision Language Navigation Using Hierarchical Chunk Memory Attention and Reinforcement Learning with Human Feedback</title><link href="http://localhost:4000/research_projects/2023-01-10-VLN-with-HCAM-RLHF/" rel="alternate" type="text/html" title="Vision Language Navigation Using Hierarchical Chunk Memory Attention and Reinforcement Learning with Human Feedback" /><published>2023-01-10T00:00:00-05:00</published><updated>2023-09-09T17:49:31-04:00</updated><id>http://localhost:4000/research_projects/VLN-with-HCAM-RLHF</id><content type="html" xml:base="http://localhost:4000/research_projects/2023-01-10-VLN-with-HCAM-RLHF/"><![CDATA[<hr />

<h2 id="results-go-to-github-and-run-the-below-commandfor-ui">Results go to <a href="https://github.com/shivacharan22/VLN-Using-Hierarchical-Chunk-Memory-Attention-and-RLHF">github</a> and run the below command(for UI)</h2>
<blockquote>
  <p>streamlit run app.py</p>
</blockquote>

<h2 id="problem-and-summary-of-the-project">Problem and summary of the project</h2>

<p>Vision language navigation is a task in Embodied AI research where an agent has to navigate to a location mentioned given language instruction and visual inputs. However, the vision language navigation task poses some challenges, like Visual Complexity, Language Ambiguity, Multimodal Integration, Generalization to Unseen Environments, Dealing with Uncertainty, Zero-shot Learning, Real-time Decision-making, Evaluation Metrics, Human- Agent Interaction, and Long-Term Dependencies. In this project, we concentrate on Human- Agent Interaction and Long-Term Dependencies Challenges for which we introduce HCAM and RLHF mechanisms to our model and training, which enable the agent to think like a human while navigating and storing to infer to make future informed decisions in the same episode as cross episode memory is not tested in this project. We use the Mattersim simulator for an environment with 1.2 TB of data. The dataset comprises 194,400 RGB-D images of 90 building-scale scenes with 21,567 navigation instructions. Our model will be trained using contrastive supervised learning with ground actions, and RLHF techniques will be used as mentioned below. We observe its success in Human-Agent Interaction and Long-Term dependencies and failure in Generalization to Unseen Environments. The metrics used are success rate 42%, Trajectory length - x, and Navigation error - Y.</p>

<h2 id="intution">Intution</h2>

<p><img src="/assets/img/arvl.jpeg" alt="The layout of the whole network" /></p>

<h3 id="why-hcam">why HCAM</h3>

<p>Regarding Long-Term Dependencies, VLN tasks often require agents to remember and refer to previous states, actions, or instructions to make informed navigation choices. Capturing and utilizing long-term dependencies effectively is a challenge, particularly when memory and context span over extended periods. HCAM introduced by can be experimented upon in this case. The original paper presents a novel approach called “hierarchical memory” HCAM to enhance the decision-making capabilities of RL agents. To evaluate the effectiveness of hierarchical memory, the authors conduct experiments, including Remembering the Ballet, which involves the agent recalling its previous observations and going to that particular dancer after seeing a 12-32 dance episode. Memory and recall were tested in all of these experiments. The results demonstrate that agents equipped with hierarchical memory outperform those without memory or with a single-level memory.
Furthermore, the hierarchical memory allows the agents to generalize their knowledge across different tasks and adapt their decision-making strategies to unseen situations. Overall, the paper presents the concept of hierarchical memory as a valuable mechanism for RL agents to improve their decision-making abilities. By enabling agents to perform mental time travel and leverage past experiences, the hierarchical memory enhances the agents’ ability to generalize, adapt, and make informed decisions in dynamic environments. We will be using this mechanism in this paper.</p>

<p><img src="/assets/img/Hcam.png" alt="HCAM" /></p>

<h3 id="why-rlhf">why RLHF</h3>

<p>The next part of this paper uses RLHF. The concept of Reinforcement Learning with Human Feedback (RLHF) has been explored in various studies, but pinpointing the exact first paper on RLHF can be challenging due to the vast literature in the field. However, one influential early article on RLHF is “Apprenticeship Learning via Inverse Reinforcement Learning” by Pieter Abbeel and Andrew Y. Ng. In this paper, the authors introduce the concept of apprenticeship learning, which combines ideas from inverse reinforcement learning (IRL) and RLHF. The focus is on learning from demonstrations provided by an expert rather than relying solely on trial-and-error exploration. By incorporating human demonstrations, the RL agent can learn from an expert’s behavior and acquire skills more efficiently. The paper proposes a mathematical framework for apprenticeship learning, which involves estimating the underlying reward function from expert demonstrations and then using this estimated reward function to train the RL agent.
RLHF was initially introduced as an approach to address the challenges of sparse reward signals and accelerate the learning progress in RL. Traditional RL methods rely on trial-and- error exploration to learn optimal policies, which can be time-consuming and inefficient. RLHF leverages human expertise to provide additional guidance and feedback to the RL agent, helping it learn faster and perform better. In RLHF, human feedback can take various forms, such as demonstrations, preferences, or direct evaluations. Demonstrations involve a human expert showcasing desired behaviors or providing example trajectories for the RL agent to learn from. Preferences involve the comparison of different actions or trajectories to indicate preferred choices. Direct evaluations provide explicit feedback or reward signals to guide the learning process. For example, we generate multiple trajectories, manually compare them in two pairs, and select the most successful one.</p>

<p><img src="/assets/img/Blank diagram (4).png" alt="Step-2" /></p>

<p>An additional step-2 was to run with contrastive learning on the whole network with simulated game plays with langauge and image pairs as shown above.</p>

<h2 id="successful-navigation-sample">successful navigation sample:</h2>
<p>https://github.com/shivacharan22/shivacharan22.github.io/assets/54499416/5c2073a6-a176-4f40-8d33-e9abcbf1e5f2</p>

<h2 id="unsuccessful-navigation-sample">Unsuccessful navigation sample:</h2>
<p>https://github.com/shivacharan22/VLN-Using-Hierarchical-Chunk-Memory-Attention-and-RLHF/blob/main/videos/val_v2.mp4</p>

<h2 id="for-detailed-information-please-check-out-this-report">For detailed information please check out this <a href="https://github.com/shivacharan22/shivacharan22.github.io/files/12554322/Capstone_report_upload_.pdf">!Report!</a></h2>

<h2 id="appendix">Appendix:</h2>

<h3 id="setup-i-used-to-run">Setup I used to run:</h3>

<p>I used GCP VM with 104 gb RAM with A100 GPU and 32 CPUs with Ubantu18.05 OS.</p>

<p>Below are the commands if you wanna fasten your process:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb:

#This command uses wget to download a Debian package (cuda-keyring) from an NVIDIA repository. The package is used to authenticate CUDA-related software.
sudo dpkg -i cuda-keyring_1.0-1_all.deb:

#This command installs the previously downloaded Debian package (cuda-keyring) using dpkg. This package likely contains GPG keys and other authentication-related data for NVIDIA's CUDA repository.
sudo apt-get update:

#This command updates the package lists for the APT package manager, ensuring that you have the latest information about available packages and their versions.
sudo apt-get -y install cuda:

#This command installs CUDA, which is a parallel computing platform and application programming interface (API) developed by NVIDIA. CUDA is commonly used for GPU-accelerated computations.
sudo apt update:

#This command updates the package lists again.
sudo apt install apt-transport-https ca-certificates curl software-properties-common:

#This command installs various utilities and libraries necessary for handling HTTPS, certificates, and adding software repositories.
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -:

#This command fetches the Docker GPG key and adds it to the list of trusted keys on your system. It's used to verify the authenticity of Docker packages.
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable":

#This command adds the official Docker repository to your system's list of software sources.
sudo apt update:

#This command updates the package lists again to include the Docker repository.
sudo apt-cache policy docker-ce:

#This command checks the available versions of Docker in the newly added repository.
sudo apt install docker-ce:

#This command installs Docker Community Edition (CE), a containerization platform.
sudo systemctl status docker:

#This command checks the status of the Docker service, confirming that it is running.
sudo usermod -aG docker ${USER}:

#This command adds your user account to the "docker" group, allowing you to run Docker commands without using "sudo."
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -:

#This command fetches the NVIDIA Docker GPG key and adds it to your system's trusted keys.
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list:

#This command adds the NVIDIA Docker repository to your system's list of software sources.
sudo apt-get update:

#This command updates the package lists once more to include the NVIDIA Docker repository.
sudo apt-get install -y nvidia-docker2:

#This command installs NVIDIA Docker 2, which enables you to use NVIDIA GPUs with Docker containers.
sudo pkill -SIGHUP dockerd:

#This command sends a signal to the Docker daemon (dockerd) to reload its configuration. This is often necessary after installing Docker-related software.
xhost +:

#This command allows any user to connect to your X server (GUI) temporarily.
nvidia-docker run -it -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix --mount type=bind,source=$MATTERPORT_DATA_DIR,target=/root/mount/Matterport3DSimulator/data/v1/scans,readonly --volume pwd:/root/mount/Matterport3DSimulator mattersim:9.2-devel-ubuntu18.04:

#This is a complex command that appears to run a Docker container named "mattersim:9.2-devel-ubuntu18.04." The container is configured to use NVIDIA Docker, share the X server display, and mount specific directories as volumes.
cd /root/mount/Matterport3DSimulator:

#This command changes the current directory to "/root/mount/Matterport3DSimulator."
git clone --recursive https://github.com/peteanderson80/Matterport3DSimulator.git:

#This command clones a Git repository named "Matterport3DSimulator" and its submodules from the specified URL.
</code></pre></div></div>]]></content><author><name>shivacharan oruganti</name><email>shivaco@bu.edu</email></author><category term="Research_projects" /><summary type="html"><![CDATA[Vision Language Navigation Using Hierarchical Chunk Memory Attention and Reinforcement Learning with Human Feedback]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/model.jpeg" /><media:content medium="image" url="http://localhost:4000/assets/img/model.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Hybrid Network Combining Cnn and Transformer Encoder to Classify Mosquitoes Based on Wing Beat Frequencies</title><link href="http://localhost:4000/research_projects/2022-08-01-HYD_NN/" rel="alternate" type="text/html" title="A Hybrid Network Combining Cnn and Transformer Encoder to Classify Mosquitoes Based on Wing Beat Frequencies" /><published>2022-08-01T00:00:00-04:00</published><updated>2023-09-09T17:49:31-04:00</updated><id>http://localhost:4000/research_projects/HYD_NN</id><content type="html" xml:base="http://localhost:4000/research_projects/2022-08-01-HYD_NN/"><![CDATA[<hr />
<h2 id="the-reason-for-this-project">The reason for this project</h2>
<p>Whenever I visit places with mosquitoes around, I am the one who gets bitten a lot compared to others (My blood is tasty). At this time, I was taking my higher-level undergraduate classes as a junior in college, where I had to come up with project ideas for my courses. My brain whispered, “Why not make a machine to kill all the bloody mosquitoes?”. The first thing I looked up was, will the world survive without mosquitoes? The answer is absolutely not! So killing only which bit me seems fair. Hence, the machine should be able to detect mosquitoes and kill them somehow. The detection part is okay, but the killing part by automation seems a bit dangerous as pointing lasers can be harmful if false positives occur in the system that can hurt humans. Finally, after much thought, I had to settle for building the first part of my idea and leave the killing to the mosquito-killing lamps. (You will be thrilled to know that the first part is as important if not more important than building the second one as you read this post.)</p>

<h2 id="global-problem">Global problem</h2>
<p>Check out the abstract of the paper. I promise its just few lines.</p>

<h2 id="the-process">The process</h2>

<p class="message"><strong>NOTE</strong>:I am just going to present different parts of the project on the surface and important ones a bit deep. If you wanna know the whole thing please go to the paper where it is clearly explained or reach out to me:)</p>
<ol>
  <li>Research<br />
The first task is to find out whats the current praticies and the history of killing I mean playing with mosquitoes.</li>
</ol>

<blockquote>
  <p><strong><em>“There is no better teacher than history in determining the future. There are decades where nothing happens, and there are weeks where decades happen.”</em></strong> – Charlie Munger and Vladimir Lenin</p>
</blockquote>

<p>All of the research about mosquito detection/classification conducted can be catagoried into three parts which are Geospatial-based Approaches, Visual based Approaches, and Audio based Approaches. Although the studies I refered in the papar showed impressive results, the authors expressed that
the data collected is informative but costly to collect, and places one wants to predict mosquito abundance might not have such
detailed data as such places are socioeconomically disadvantaged.</p>

<p><a href="https://elifesciences.org/articles/27854">8</a>shows that it is possible to make high-quality Mosquito wingbeat recordings with ordinary smartphones, even with a noisy background. Since mosquitoes rarely fly faster than one meter per second, they also discovered that the Doppler effect is insignificant for such recordings, with a recording error of about 2 Hz for mosquitoes up to 10 cm away from the microphone.</p>

<ol>
  <li>Data preprocessing<br />
I have taken the dataset from the study[8]. It has 1381 files, including all 23 species of mosquitoes.
 Most audio files
had 8Khz as the sample rate, and the remaining files had 44.1Khz and 48Khz as sampling rates. Audio durations range from 0 to
600 seconds for the files. The total duration of all the files is around 33k seconds. Used Torchaudio for data analysis.</li>
</ol>

<p>first, we convert all the files into WAV format, then we manually remove any part of the
video that is not the wingbeat(other sounds or no sound), and previous studies removed noisy wingbeat audio clips, which we did
not do in this analysis as we believe learning to detect wingbeats in noisy backgrounds will be a helpful feature in the actual
application of the Model in real-time. Second, we divided the audio clips into 1.5-second chunks without overlapping such that
no same information gets shared among train and test data. Last, we downsample all the audio files to 8Khz, the lowest in the
dataset, in accordance with Nyquist–Shannon sampling theorem used by previous works<a href="https://elifesciences.org/articles/27854">8</a>.</p>

<ol>
  <li>Approach:<br />
Our approach consists of CNNs and transformer encoders. CNN’s structure is inspired by the human brain’s primary visual
cortex(V1). CNN layers are designed to capture the functions of simple and complex cells in V1. The function of simple cells can
be best described as a linear function of the input in spatially confined receptive field<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8010506/">26</a>. The detector units in CNN have this function. On the other hand, Complex cells are resilient to little changes in the feature’s position. Pooling units of CNNs have the same function. Sparse interactions, parameter sharing, and equivariant representations, these essential properties of CNN that make them more computationally efficient and performance boosters in various tasks<a href="https://www.deeplearningbook.org/">27</a>. It is shown that CNN has the extraordinary ability to detect patterns in images and Audio.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">conv_block</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">conv_bias</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">spin_conv</span><span class="p">():</span>
    <span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">conv_bias</span><span class="p">)</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">kaiming_normal_</span><span class="p">(</span><span class="n">conv</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conv</span>
  <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                    <span class="nf">spin_conv</span><span class="p">(),</span>
                    <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
                    <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                        <span class="nc">TransposeLast</span><span class="p">(),</span>
                        <span class="nc">Fp32LayerNorm</span><span class="p">(</span><span class="n">n_out</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                        <span class="nc">TransposeLast</span><span class="p">(),</span>
                    <span class="p">),</span>
                    <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
                <span class="p">)</span>
<span class="k">def</span> <span class="nf">make_conv_layers</span><span class="p">():</span>
        <span class="n">in_d</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">conv_dim</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span><span class="o">*</span><span class="mi">4</span> <span class="o">+</span> <span class="p">[(</span><span class="mi">256</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[(</span><span class="mi">256</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)]</span>
        <span class="n">conv_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cl</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">conv_dim</span><span class="p">):</span>
            <span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span> <span class="o">=</span> <span class="n">cl</span>
            <span class="n">conv_layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="nf">conv_block</span><span class="p">(</span>
                    <span class="n">in_d</span><span class="p">,</span>
                    <span class="n">dim</span><span class="p">,</span>
                    <span class="n">k</span><span class="p">,</span>
                    <span class="n">stride</span><span class="p">,</span>
                    <span class="n">conv_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">in_d</span> <span class="o">=</span> <span class="n">dim</span> 
        <span class="k">return</span> <span class="n">conv_layers</span>
</code></pre></div></div>

<p>Model contains a multi-layer convolutional feature encoder f: X-¿Z, which takes raw Audio as input and outputs latent
representations(Z). After adding positional embeddings, these representations go into transformer encoder g: Z-
¿C as shown in the fig1, concatenated with the classification token, to get global context vectors capturing information about the
sequence, then the classification vector output is sent to a fully connected network for classification.
The convolutional feature encoder has seven layers. Each layer has three components which are conv1d,
layer normalization, and activation function GELU. Each layer has a different kernel size and stride value, as shown in table2. We
use a dropout value of 0.1 during training. Most configurations are the same as Wav2vec2.0[22]. We also use a transformer in our
Model. A transformer consists of an encoder and a decoder; as our Model is designed for the classification task, we use a
Transformer encoder-only architecture which contains multi-head attention, residual connections, feed-forward layers, and layer
normalization, the same as BERT[29]With classification token appended at the end rather than the usual starting position in BERT. We have two encoder layers with four heads in each and
a dimension of 2048 for feed-forward networks using a dropout of 0.1. The last classification has 23 outputs, one for each species.
These three parts are connected as shown in Table Fig1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">wavmosLit</span><span class="p">(</span><span class="n">pl</span><span class="p">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span>
        <span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">wavmosLit</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_all</span> <span class="o">=</span> <span class="nc">Precision</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_global</span> <span class="o">=</span> <span class="nc">Precision</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">macro</span><span class="sh">'</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_weighted</span> <span class="o">=</span> <span class="nc">Precision</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">weighted</span><span class="sh">'</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">23</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_each</span> <span class="o">=</span> <span class="nc">Precision</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">23</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">recall_all</span> <span class="o">=</span> <span class="nc">Recall</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_global</span> <span class="o">=</span> <span class="nc">Recall</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">macro</span><span class="sh">'</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_weighted</span> <span class="o">=</span> <span class="nc">Recall</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">weighted</span><span class="sh">'</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">23</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_each</span> <span class="o">=</span> <span class="nc">Recall</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">23</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">f1</span> <span class="o">=</span> <span class="nc">F1Score</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">confmat</span> <span class="o">=</span> <span class="nc">ConfusionMatrix</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">valid_acc</span> <span class="o">=</span> <span class="n">torchmetrics</span><span class="p">.</span><span class="nc">Accuracy</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">23</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">valid_acc_each</span> <span class="o">=</span>  <span class="n">torchmetrics</span><span class="p">.</span><span class="nc">Accuracy</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="sh">"</span><span class="s">multiclass</span><span class="sh">"</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">23</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">inner_dim</span> <span class="o">=</span> <span class="mi">49</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">TransformerEncoder</span><span class="p">()</span>
        <span class="c1">#self.cls_emb = nn.Linear(1,49)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">last_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">inner_dim</span><span class="p">,</span>
            <span class="mi">23</span>
            <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_global_shared_layer</span> <span class="o">=</span> <span class="n">TORCH_GLOBAL_SHARED_LAYER</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_output</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_output</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">_global_shared_layer</span><span class="p">:</span>
             <span class="n">self</span><span class="p">.</span><span class="n">_output</span> <span class="o">=</span> <span class="nf">conv</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_output</span><span class="p">)</span>
        <span class="c1">#self.temp = torch.ones([self._output.shape[0],1,49],device = device)
</span>        <span class="c1">#self.cls_token_emb = self.cls_emb(torch.tensor([50], dtype=torch.float,device = device))
</span>        <span class="c1">#self.temp[:,0,:] = self.cls_token_emb
</span>        <span class="n">self</span><span class="p">.</span><span class="n">_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">_output</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">_output</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">,</span><span class="mi">49</span><span class="p">],</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">)),</span><span class="mi">1</span><span class="p">)</span>                         
        <span class="n">self</span><span class="p">.</span><span class="n">_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_output</span><span class="p">)</span>
        <span class="n">model_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">last_layer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_output</span><span class="p">[:,</span><span class="mi">256</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">model_out</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span>  <span class="nf">loss_function</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">stage</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>

        <span class="n">train_ids</span><span class="p">,</span><span class="n">test_ids</span> <span class="o">=</span> <span class="n">spilts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">training_data</span> <span class="o">=</span> <span class="nc">CustomImageDataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">train_subsampler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">SubsetRandomSampler</span><span class="p">(</span><span class="n">train_ids</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">test_subsampler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">SubsetRandomSampler</span><span class="p">(</span><span class="n">test_ids</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    
        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
                      <span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">,</span> 
                      <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">train_subsampler</span><span class="p">,</span><span class="n">num_workers</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span><span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">train_loader</span>

    <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    
        <span class="n">val_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
                      <span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">,</span>
                      <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">test_subsampler</span><span class="p">,</span><span class="n">num_workers</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span><span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">val_loader</span>
    
    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span>  <span class="nf">loss_function</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">valid_acc</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">valid_acc_each</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_all</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_global</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_weighted</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_each</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_all</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_global</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_weighted</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_each</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">confmat</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">val_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">]).</span><span class="nf">mean</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="sh">"</span><span class="s">T_avg_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">avg_loss</span><span class="p">,</span><span class="n">prog_bar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="sh">"</span><span class="s">T_avg_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">avg_loss</span><span class="p">})</span>
        
    <span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">]).</span><span class="nf">mean</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="sh">"</span><span class="s">val_loss</span><span class="sh">"</span><span class="p">,</span><span class="n">avg_loss</span><span class="p">,</span><span class="n">prog_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="sh">"</span><span class="s">val_acc</span><span class="sh">"</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">valid_acc</span><span class="p">.</span><span class="nf">compute</span><span class="p">(),</span><span class="n">prog_bar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="sh">"</span><span class="s">val_loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">avg_loss</span><span class="p">,</span>
                   <span class="sh">'</span><span class="s">val_acc</span><span class="sh">'</span><span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">valid_acc</span><span class="p">.</span><span class="nf">compute</span><span class="p">(),</span>
                  <span class="sh">"</span><span class="s">recall_all</span><span class="sh">"</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">recall_all</span><span class="p">.</span><span class="nf">compute</span><span class="p">(),</span>

                  <span class="sh">"</span><span class="s">recall_global</span><span class="sh">"</span><span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">recall_global</span><span class="p">.</span><span class="nf">compute</span><span class="p">(),</span>
                  <span class="sh">"</span><span class="s">recall_weighted</span><span class="sh">"</span><span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">recall_weighted</span><span class="p">.</span><span class="nf">compute</span><span class="p">(),</span>
        
                  <span class="sh">"</span><span class="s">precision_all</span><span class="sh">"</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">precision_all</span><span class="p">.</span><span class="nf">compute</span><span class="p">(),</span>
                  <span class="sh">"</span><span class="s">precision_global</span><span class="sh">"</span><span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">precision_global</span><span class="p">.</span><span class="nf">compute</span><span class="p">(),</span>
                  <span class="sh">"</span><span class="s">precision_weighted</span><span class="sh">"</span><span class="p">:</span><span class="n">self</span><span class="p">.</span><span class="n">precision_weighted</span><span class="p">.</span><span class="nf">compute</span><span class="p">(),</span>
    
                  <span class="c1">#"confusion_matrix": self.confmat.compute()
</span>                  <span class="p">})</span>
        <span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="sa">f</span><span class="sh">"</span><span class="s">test_acc_each/acc-</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">loss</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">valid_acc_each</span><span class="p">.</span><span class="nf">compute</span><span class="p">())})</span>
        <span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="sa">f</span><span class="sh">"</span><span class="s">recall_each/recall_each-</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">loss</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">precision_each</span><span class="p">.</span><span class="nf">compute</span><span class="p">())})</span>
        <span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="sa">f</span><span class="sh">"</span><span class="s">precision_each/precision_each-</span><span class="si">{</span><span class="n">ii</span><span class="si">}</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span> <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">loss</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">precision_each</span><span class="p">.</span><span class="nf">compute</span><span class="p">())})</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">training_data</span><span class="p">.</span><span class="n">lab_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">confmat</span><span class="p">.</span><span class="nf">compute</span><span class="p">())</span>
        <span class="n">self</span><span class="p">.</span><span class="n">confmat</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">valid_acc</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">valid_acc_each</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_all</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_global</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_weighted</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">recall_each</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_all</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_global</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_weighted</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">precision_each</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">avg_loss</span><span class="p">}</span><span class="c1">#, 'log': tensorboard_logs}
</span>    
    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">)),</span><span class="n">capturable</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="nc">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">[</span><span class="mi">60</span><span class="p">],</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">optimizer</span><span class="p">],</span> <span class="p">[</span><span class="n">scheduler</span><span class="p">]</span>
</code></pre></div></div>

<p>To evaluate our approach, we choose the PyTorch framework to train the Model:
1.We ran a hyperparameter search on the learning rate, revealing 0.0004 to be suitable.
2.We analyzed batch sizes starting from 32 to 128 in multiples of 32 and found that size 64 gives the best performance,
whereas the batch size of 128 resulted in complete hardware capacity utilization.
3.The best-performing version used 64 as batch size and 2 ∗(0.0004) as the learning rate.
We trained on every fold for 80 epochs before testing. Model parameters were optimized by Adam optimizer with standard
configuration. The Model also used the LR scheduler with a milestone at the 60th epoch decaying by 0.1. The loss function used
is Cross Entropy Loss, as it is used in training multi-class problems.</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Precision</th>
      <th>Recall</th>
      <th>Accuracy</th>
      <th>Precision PW</th>
      <th>PM</th>
      <th>Recall RW</th>
      <th>RM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Average</td>
      <td>0.8127</td>
      <td>0.8127</td>
      <td>0.8127</td>
      <td>0.7209</td>
      <td>0.8127</td>
      <td>0.8127</td>
      <td>0.7093</td>
    </tr>
    <tr>
      <td>SD</td>
      <td>0.0093</td>
      <td>0.0093</td>
      <td>0.0093</td>
      <td>0.0456</td>
      <td>0.0093</td>
      <td>0.0093</td>
      <td>0.0316</td>
    </tr>
  </tbody>
</table>

<p>The hybrid model achieved an overall 81.27%(±0.93) accuracy with Precision and Recall equalling accuracy up to six decimal
points. Recall weighed also equals with Recall and differs in standard deviation about (±0.9311), and Precision weighted achieved
81.70%(±1.02), Macro version of the precision score achieved is 72.09%(±4.56), Recall macro version percentage is
70.93%(±3.16) showcasing its effectiveness in classifying mosquito species based on wingbeat sounds. Notably, both Precision and Recall closely matched this accuracy up to six decimal points, indicating precise performance. The model excelled in identifying Anopheles minimus and Culiseta incidens due to data availability and unique wingbeat frequency distributions. However, it struggled with Anopheles quadrimaculatus and Aedes mediovittatus, which had limited data. Despite some misclassifications, the model showed improvement in distinguishing species with overlapping wingbeat frequencies which was a big problem in previous models. Overall, its robust and balanced metrics, with standard deviations below 3%, demonstrate its reliability as a classifier.</p>

<p class="figure"><img src="/assets/img/result_f1_acc.png" alt="Full-width image" class="lead" width="800" height="100" />
A caption to an image.</p>

<p class="figure"><img src="/assets/img/result_f1_acc_sd.png" alt="Full-width image" class="lead" width="800" height="100" />
A caption to an image.</p>

<p>Importantly, the proposed multi-class model’s accuracy surpassed that of previous studies that used spatial information, achieving higher accuracy levels, even though it relied solely on raw acoustic data from mosquito wingbeats. This model outperformed studies with accuracy levels of 65%, 78.12%, and 80%, showcasing its remarkable performance in mosquito species classification.</p>

<p class="figure"><img src="/assets/img/result_recall_prec_avg.png" alt="Full-width image" class="lead" width="800" height="100" />
A caption to an image.</p>

<p class="figure"><img src="/assets/img/result_recall_prec_std.png" alt="Full-width image" class="lead" width="800" height="100" />
A caption to an image.</p>

<p class="figure"><img src="/assets/img/results_confu_matrix.png" alt="Full-width image" class="lead" width="800" height="100" />
A caption to an image.</p>]]></content><author><name>shivacharan oruganti</name><email>shivaco@bu.edu</email></author><category term="Research_projects" /><summary type="html"><![CDATA[A Hybrid Network Combining Cnn and Transformer Encoder to Classify Mosquitoes Based on Wing Beat Frequencies]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/blog/arc1.png" /><media:content medium="image" url="http://localhost:4000/assets/img/blog/arc1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The problem</title><link href="http://localhost:4000/research_projects/2022-07-11-learning-embedding/" rel="alternate" type="text/html" title="The problem" /><published>2022-07-11T00:00:00-04:00</published><updated>2023-09-07T21:45:26-04:00</updated><id>http://localhost:4000/research_projects/learning-embedding</id><content type="html" xml:base="http://localhost:4000/research_projects/2022-07-11-learning-embedding/"><![CDATA[<p>In the competition, we were asked to develop models that can efficiently retrieve images from a large database. For each query image, the model is expected to retrieve the most similar images from an set hidden to us.</p>

<p>In their words</p>
<blockquote>
  <p>“In this competition, the developed models are expected to retrieve relevant database images to a given query image (ie, the model should retrieve database images containing the same object as the query). The images in our dataset comprise a variety of object types, such as apparel, artwork, landmarks, furniture, packaged goods, among others.”</p>
</blockquote>

<h2 id="process">Process</h2>

<ul>
  <li>
    <p>First thing is we are not given a dataset so we have to make a dataset from public use licenced datasets only*. So I have prepared a datset of 25k images with more or less equal ratios of the provided catogaries. Here are the sources from which the dataset was made:</p>
  </li>
  <li>
    <ol>
      <li>https://www.kaggle.com/datasets/naturalseeker/triplet-data https://www.kaggle.com/datasets/abhijeetbhilare/world-cuisines</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>https://www.kaggle.com/datasets/naturalseeker/packed-goods</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>https://www.kaggle.com/datasets/naturalseeker/packed-goods</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>https://www.kaggle.com/datasets/naturalseeker/images-for-google-comp</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>https://www.kaggle.com/datasets/watts2/glove6b50dtxt</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>https://www.kaggle.com/datasets/naturalseeker/door-image</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>https://www.kaggle.com/datasets/jaidevchittoria/babies-products-and-toys</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>https://www.kaggle.com/competitions/google-universal-image-embedding</li>
    </ol>
  </li>
  <li>Next I have to select a training method its an obivous choice to fine tune already available models and use triplet loss. I used Swin Transformer and DeiT. Swin transformer shined but I had to freeze the layers when finetuning.</li>
</ul>

<p>check out the project github <a href="https://github.com/shivacharan22/Learning-Embedding-for-images-using-Deep-leaning/tree/main">here</a></p>

<p>Together 100k pair images from 25k initial images.</p>

<h2 id="results-">Results :</h2>
<p>95 % accuracy on test data(10%) separated from above.</p>

<h3 id="test-sample">Test sample:</h3>

<p><img width="517" alt="Screenshot 2023-01-12 at 3 20 01 AM" src="https://user-images.githubusercontent.com/54499416/211925349-3036f946-3ba5-49b3-9172-279046938351.png" /></p>

<ul>
  <li>
    <p>Image original 
 <img width="274" alt="image" src="https://user-images.githubusercontent.com/54499416/211925837-592b071a-f56e-477e-b95a-e8f7a1bd5417.png" /></p>
  </li>
  <li>Image 1 comparision positive score of <strong>3.39</strong>
<img width="317" alt="image" src="https://user-images.githubusercontent.com/54499416/211926371-7a9e4def-9931-4dab-8454-3812d5b1b7ad.png" /></li>
  <li>Image 2 comparision Negitive score of 11.35
<img width="222" alt="image" src="https://user-images.githubusercontent.com/54499416/211926822-76b11449-b691-4f99-9af1-9caf2562e5fa.png" /></li>
</ul>

<p>Model predicts Image 1 is more similar to original than Image 2.</p>]]></content><author><name>shivacharan oruganti</name><email>shivaco@bu.edu</email></author><category term="Research_projects" /><summary type="html"><![CDATA[Google Universal Image Embedding Kaggle competition]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/learn_emb.png" /><media:content medium="image" url="http://localhost:4000/assets/img/learn_emb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Volatility-prediction</title><link href="http://localhost:4000/research_projects/2021-06-23-Realized-Volatility/" rel="alternate" type="text/html" title="Volatility-prediction" /><published>2021-06-23T00:00:00-04:00</published><updated>2023-09-07T21:50:54-04:00</updated><id>http://localhost:4000/research_projects/Realized%20Volatility</id><content type="html" xml:base="http://localhost:4000/research_projects/2021-06-23-Realized-Volatility/"><![CDATA[<hr />

<p>Check out the github: <a href="https://github.com/shivacharan22/Volatility-prediction">here</a></p>]]></content><author><name>shivacharan oruganti</name><email>shivaco@bu.edu</email></author><category term="Research_projects" /><summary type="html"><![CDATA[Optiver Realized Volatility Prediction Kaggle competition]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/brandbird.png" /><media:content medium="image" url="http://localhost:4000/assets/img/brandbird.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>